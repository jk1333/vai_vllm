{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e34c65-52e5-43c6-83ef-e77f433274cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For downloading model to private storage. Use'll use cloud storage as model repository. Tuned model should have uploaded to cloud storage\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "hf_token = \"\"\n",
    "hf_repo_id = \"OpenGVLab/InternVL2-8B\"\n",
    "BASE_ARTIFACT_URI = \"gs://jk-model-repo\"\n",
    "cache = \"/home/jupyter/model\"\n",
    "os.system(f\"rm -rf {cache}\")\n",
    "print(\"Start downloading\")\n",
    "snapshot_download(repo_id=hf_repo_id, token=hf_token, local_dir=cache)\n",
    "print(\"Uploading\")\n",
    "os.system(f\"gcloud storage cp {cache}/*.* {BASE_ARTIFACT_URI}/{hf_repo_id}\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b856565-a5f5-4f28-9940-50671fefbe17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Basic configuration\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "LOCATION = \"asia-northeast1\" # For v6e, asia-northeast1 | For v5e, , us-south1\n",
    "PROJECT_NUMBER = \"1045259343465\"\n",
    "#Beware that nightly version changes a lot and invalid image also uploaded. Need to fix a version before testing\n",
    "#vLLM_DOCKER_URI = \"asia-northeast1-docker.pkg.dev/sandbox-373102/custom-inference-tpu/vllm-tpu:latest\"\n",
    "#vLLM_DOCKER_URI = \"asia-northeast1-docker.pkg.dev/sandbox-373102/custom-inference-tpu/vllm-tpu:custom\"\n",
    "#vLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250819_0917_tpu_experimental_RC01\"\n",
    "vLLM_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/vllm-inference-tpu.0-11.ubuntu2204.py312:model-garden.vllm-tpu-release_20260127.00_p0\"\n",
    "DEPLOY_TIMEOUT = 3600"
   ]
  },
  {
   "cell_type": "raw",
   "id": "639b413c-4f27-4b0e-92c5-55f5fce898e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Run if the container is custom built\n",
    "!docker build -t {vLLM_DOCKER_URI} .\n",
    "!docker image push {vLLM_DOCKER_URI}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9164d21-ce50-4c3c-a2cb-c755eec9aa10",
   "metadata": {},
   "source": [
    "#Run if the container is custom built or outside of google cloud\n",
    "!gcloud auth configure-docker {LOCATION}-docker.pkg.dev --quiet\n",
    "!docker pull vllm/vllm-tpu:87f48623a537d379284bb3e3d1b23ab0ee2af1c1\n",
    "!docker image tag vllm/vllm-tpu:87f48623a537d379284bb3e3d1b23ab0ee2af1c1 {vLLM_DOCKER_URI}\n",
    "!docker image push {vLLM_DOCKER_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e8a7f-5004-4379-bc83-05e631c803c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"1045259343465-compute@developer.gserviceaccount.com\"   #SA to access gcs\n",
    "#Model configuration\n",
    "MODEL_NAME_V1 = \"Llama-3.3 70B TPU\"\n",
    "MODEL_PATH_V1 = \"gs://jk-model-repo/meta-llama/Llama-3.3-70B-Instruct\"\n",
    "#BASE_PATH = \"/gcs/jk-model-repo\"\n",
    "#MODEL_PATH_V1 = \"google/gemma-3-27b-it\"\n",
    "TENSOR_PARALLEL_SIZE = 8\n",
    "#MAX_MODEL_LEN = 128 * 1024\n",
    "MAX_MODEL_LEN = 2048\n",
    "MACHINE_TYPE = f\"ct6e-standard-{TENSOR_PARALLEL_SIZE}t\" # ct6e-standard-8t, ct5lp-hightpu-8t\n",
    "TPU_TOPOLOGY = None #Omit the topology to host on single machine, None, 2x2, 2x4\n",
    "ACCELERATOR_TYPE = None\n",
    "ACCELERATOR_COUNT = None\n",
    "\n",
    "#MODEL_NAME_V1 = \"Llama-3.1-8B\"\n",
    "#MODEL_PATH_V1 = \"gs://jk-model-repo/meta-llama/Llama-3.1-8B\"\n",
    "#TENSOR_PARALLEL_SIZE = 1\n",
    "#MAX_MODEL_LEN = 32768\n",
    "#MACHINE_TYPE = \"a2-highgpu-1g\"\n",
    "#ACCELERATOR_TYPE = \"NVIDIA_TESLA_A100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681467cc-60ad-4b07-a545-5a9a00a19b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"1045259343465-compute@developer.gserviceaccount.com\"   #SA to access gcs\n",
    "#Model configuration\n",
    "MODEL_NAME_V1 = \"Gemma-3-27B TPU\"\n",
    "MODEL_PATH_V1 = \"gs://jk-model-repo/google/gemma-3-27b-it\"\n",
    "TENSOR_PARALLEL_SIZE = 4\n",
    "MAX_MODEL_LEN = 32768\n",
    "MACHINE_TYPE = f\"ct6e-standard-{TENSOR_PARALLEL_SIZE}t\" # ct6e-standard-8t, ct5lp-hightpu-8t\n",
    "TPU_TOPOLOGY = None #Omit the topology to host on single machine, None, 2x2, 2x4\n",
    "ACCELERATOR_TYPE = None\n",
    "ACCELERATOR_COUNT = None\n",
    "\n",
    "#MODEL_NAME_V1 = \"Llama-3.1-8B\"\n",
    "#MODEL_PATH_V1 = \"gs://jk-model-repo/meta-llama/Llama-3.1-8B\"\n",
    "#TENSOR_PARALLEL_SIZE = 1\n",
    "#MAX_MODEL_LEN = 32768\n",
    "#MACHINE_TYPE = \"a2-highgpu-1g\"\n",
    "#ACCELERATOR_TYPE = \"NVIDIA_TESLA_A100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6bf749-d0c2-4b6f-b180-0e734b7c933c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"1045259343465-compute@developer.gserviceaccount.com\"   #SA to access gcs\n",
    "#Model configuration\n",
    "MODEL_NAME_V1 = \"Phi-4 TPU\"\n",
    "MODEL_PATH_V1 = \"gs://jk-model-repo/microsoft/phi-4\"\n",
    "TENSOR_PARALLEL_SIZE = 2\n",
    "MAX_MODEL_LEN = 32768\n",
    "MACHINE_TYPE = f\"ct6e-standard-4t\" # ct6e-standard-8t, ct5lp-hightpu-8t\n",
    "TPU_TOPOLOGY = None #Omit the topology to host on single machine, None, 2x2, 2x4\n",
    "ACCELERATOR_TYPE = None\n",
    "ACCELERATOR_COUNT = None\n",
    "\n",
    "#MODEL_NAME_V1 = \"Llama-3.1-8B\"\n",
    "#MODEL_PATH_V1 = \"gs://jk-model-repo/meta-llama/Llama-3.1-8B\"\n",
    "#TENSOR_PARALLEL_SIZE = 1\n",
    "#MAX_MODEL_LEN = 32768\n",
    "#MACHINE_TYPE = \"a2-highgpu-1g\"\n",
    "#ACCELERATOR_TYPE = \"NVIDIA_TESLA_A100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09dafaf-43be-4169-862d-8285e4769213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Upload model pacakge to vertex model repository\n",
    "from google.cloud import aiplatform\n",
    "model_v1 = aiplatform.Model.upload(\n",
    "    location=LOCATION,\n",
    "    display_name = MODEL_NAME_V1,\n",
    "    #local_model = local_model_v1,\n",
    "    artifact_uri = MODEL_PATH_V1,\n",
    "    #parent_model = prev_model.resource_name,\n",
    "    #is_default_version=True,\n",
    "    #serving_container_environment_variables={\n",
    "    #    \"VERTEX_CPR_MAX_WORKERS\": \"1\",\n",
    "    #    \"PORT\": \"8080\", #server runs on 5000, or 8080 by dafault\n",
    "    #    \"RUST_BACKTRACE\": \"full\", #for stack trace printing,\n",
    "    #},\n",
    "    #serving_container_ports=[8080],\n",
    "    #serving_container_args = [\"--num-shard 1\"]\n",
    "    \n",
    "    #For direct container upload rather then use LocalModel\n",
    "    serving_container_image_uri=vLLM_DOCKER_URI,\n",
    "    serving_container_invoke_route_prefix=\"/*\",\n",
    "    #serving_container_predict_route=\"/v1/chat/completions\",\n",
    "    serving_container_health_route=\"/health\",\n",
    "    serving_container_ports=[8080],\n",
    "    serving_container_environment_variables={\n",
    "        #\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\",\n",
    "        \"VLLM_USE_V1\": \"1\",\n",
    "        #\"HF_TOKEN\": hf_token,\n",
    "        \"VLLM_TPU_MOST_MODEL_LEN\": \"1024\",\n",
    "        \"VLLM_TPU_BUCKET_PADDING_GAP\": \"128\"\n",
    "    },\n",
    "    serving_container_args=[\"python\",\n",
    "                            \"-m\",\n",
    "                            \"vllm.entrypoints.openai.api_server\",\n",
    "                            ##f\"--download-dir={BASE_PATH}\",\n",
    "                            #f\"--model={MODEL_PATH_V1}\",\n",
    "                            \"--port=8080\",                                               \n",
    "                            f\"--tensor-parallel-size={TENSOR_PARALLEL_SIZE}\",\n",
    "                            \"--enable-prefix-caching\",\n",
    "                            \"--enable-chunked-prefill\",\n",
    "                            f\"--max-model-len={MAX_MODEL_LEN}\",\n",
    "                            \"--served-model-name=openapi\",\n",
    "                            #\"--enable-request-id-headers\",   #enable to track id from response header\n",
    "                            \"--limit-mm-per-prompt.image=0\",\n",
    "                            \"--max-num-batched-tokens=1024\",\n",
    "                            #\"--max-num-seqs=128\",                            \n",
    "                            ],\n",
    "    serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebada3-4d82-4ae5-b5c6-5f21d4592a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Public endpoint with dedicated network\n",
    "from google.cloud import aiplatform\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=f\"{MODEL_NAME_V1} public dedicated test endpoint\",\n",
    "    labels={\"sample-key\": \"sample-value\"},\n",
    "    location=LOCATION,\n",
    "    dedicated_endpoint_enabled=True,\n",
    ")\n",
    "endpoint.deploy(\n",
    "    model = model_v1,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    tpu_topology=TPU_TOPOLOGY,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    #traffic_percentage=50\n",
    "    #traffic_split={'a':50, 'b':50}\n",
    "    #Configs for GPU\n",
    "    #accelerator_type=\"NVIDIA_L4\",\n",
    "    #machine_type=\"a2-highgpu-1g\",\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    deploy_request_timeout=DEPLOY_TIMEOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e389547-fd94-4a62-b3e7-bcfc032221f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "from google.cloud import aiplatform\n",
    "PROJECT_ID = \"sandbox-373102\"\n",
    "SECOND_PROJECT_ID = \"sandbox2-450109\"\n",
    "endpoint = aiplatform.PrivateEndpoint.create(\n",
    "    display_name=f\"{MODEL_NAME_V1} proxy psc test endpoint global access\",\n",
    "    private_service_connect_config=aiplatform.PrivateEndpoint.PrivateServiceConnectConfig(\n",
    "        project_allowlist=[PROJECT_ID, SECOND_PROJECT_ID],\n",
    "    ),\n",
    "    #network=f\"projects/{PROJECT_NUMBER}/global/networks/{VPC_NETWORK}\",\n",
    "    labels={\"sample-key\": \"sample-value\"},\n",
    "    location=LOCATION,\n",
    ")\n",
    "endpoint.deploy(\n",
    "    model = model_v1,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    tpu_topology=TPU_TOPOLOGY,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    #traffic_percentage=50\n",
    "    #traffic_split={'a':50, 'b':50}\n",
    "    #Configs for GPU\n",
    "    #accelerator_type=\"NVIDIA_L4\",\n",
    "    #machine_type=\"a2-highgpu-1g\",\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59d016f7-41d8-428f-9912-0f94c0e3a4c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "endpoint.gca_resource"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f39e1093-d7fc-47ca-ad2c-23cfe59024d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "service_attachment = endpoint.list_models()[0].private_endpoints.service_attachment\n",
    "print(service_attachment)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fce345b-78df-44e1-9c04-dd9eca955a12",
   "metadata": {
    "tags": []
   },
   "source": [
    "! gcloud compute addresses create psc-prediction-global \\\n",
    "    --region=asia-northeast1 \\\n",
    "    --subnet=subnet6"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dde49559-32be-44bb-a40f-5a94033b6f27",
   "metadata": {
    "tags": []
   },
   "source": [
    "! gcloud compute forwarding-rules create op-psc-endpoint-global \\\n",
    "    --network=globalnetwork \\\n",
    "    --address=psc-prediction-global \\\n",
    "    --target-service-attachment={service_attachment} \\\n",
    "    --region=asia-northeast1 \\\n",
    "    --allow-psc-global-access"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb546c4f-7a23-4716-a3b3-14a58fe55631",
   "metadata": {
    "tags": []
   },
   "source": [
    "IP_ADDRESS = ! gcloud compute forwarding-rules describe op-psc-endpoint-global --region=asia-northeast1 --format='value(IPAddress)'\n",
    "IP_ADDRESS = IP_ADDRESS[0]\n",
    "print(IP_ADDRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ea419-c1b6-4c89-abf3-51c493e29399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sample test\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "\n",
    "class CarType(str, Enum):\n",
    "    sedan = \"sedan\"\n",
    "    suv = \"SUV\"\n",
    "    truck = \"Truck\"\n",
    "    coupe = \"Coupe\"\n",
    "\n",
    "class CarDescription(BaseModel):\n",
    "    brand: str\n",
    "    model: str\n",
    "    car_type: CarType\n",
    "\n",
    "prediction_input = {\n",
    "    \"stream\": False,\n",
    "    \"chat_template_kwargs\": {\"enable_thinking\": False},\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Generate a JSON with the brand, model and car_type of the most iconic car from the 90's\"\n",
    "        }\n",
    "    ],\n",
    "    \"guided_json\": CarDescription.model_json_schema()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf298deb-c5a3-40fd-8b5b-6fef6d707400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Public and dedicated endpoint predict\n",
    "from google.cloud import aiplatform\n",
    "#ENDPOINT_ID = \"5931165942456254464\"\n",
    "#endpoint = aiplatform.Endpoint(ENDPOINT_ID, location=LOCATION)\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"x-request-id\": \"ebb94475-1ca2-4e4b-baa3-8d039c0e616e\", #works when --enable-request-id-headers option enabled\n",
    "    \"x-vertex-ai-timeout-ms\": \"60000\"\n",
    "}\n",
    "response = endpoint.invoke(request_path=\"/v1/chat/completions\", headers=headers, body=json.dumps(prediction_input, indent=2).encode('utf-8'),\n",
    "                          #endpoint_override=IP_ADDRESS\n",
    "                          )\n",
    "print(response.headers)\n",
    "print(response.json()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30a057-a9ca-4cb2-aae2-193110cb1860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#VLLM prometheus metric\n",
    "import google.auth\n",
    "import requests\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {creds.token}\",\n",
    "}\n",
    "url = f\"https://{ENDPOINT_ID}.{LOCATION}-{PROJECT_NUMBER}.prediction.vertexai.goog/v1/projects/{PROJECT_NUMBER}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}/invoke/metrics\"\n",
    "response = requests.post(url, data='', headers=headers)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e62d9-9dbe-4d5b-9cda-4b115af48418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For dedicated endpoint, endpoint.predict(instances=instances, use_dedicated_endpoint=True)\n",
    "#Vertex AI predict (Need to modify payload)\n",
    "import google.auth\n",
    "import requests\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "\n",
    "url = f\"https://{ENDPOINT_ID}.{LOCATION}-{PROJECT_NUMBER}.prediction.vertexai.goog/v1/projects/{PROJECT_NUMBER}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}/invoke/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {creds.token}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"x-vertex-ai-timeout-ms\": \"30000\"\n",
    "}\n",
    "payload = {\n",
    "    \"stream\": True,\n",
    "    \"chat_template_kwargs\": {\"enable_thinking\": True},\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "print(response)\n",
    "import time\n",
    "def stream_parse(response):\n",
    "    \"\"\"\n",
    "    스트리밍 Server-Sent Events (SSE) 응답을 올바르게 파싱합니다.\n",
    "\n",
    "    Args:\n",
    "        response: requests.Response 객체 (stream=True로 요청).\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    # response.iter_lines()를 사용하여 스트림을 한 줄씩 처리합니다.\n",
    "    # 이 메서드가 네트워크 청크를 자동으로 합쳐 완전한 줄을 만들어줍니다.\n",
    "    for line in response.iter_lines():\n",
    "        # keep-alive를 위한 빈 줄은 건너뜁니다.\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # bytes 타입을 string으로 디코딩합니다.\n",
    "        decoded_line = line.decode(\"utf-8\")\n",
    "\n",
    "        # SSE 데이터는 \"data:\"로 시작합니다.\n",
    "        if decoded_line.startswith(\"data:\"):\n",
    "            # \"data:\" 접두사와 양쪽 공백을 제거합니다.\n",
    "            data_str = decoded_line[5:].strip()\n",
    "\n",
    "            # API가 보내는 스트림 종료 신호입니다.\n",
    "            if data_str == \"[DONE]\":\n",
    "                break\n",
    "\n",
    "            # \"data: \" 뒤에 내용이 없는 경우를 대비합니다.\n",
    "            if not data_str:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # 문자열을 JSON 객체로 파싱합니다.\n",
    "                chunk = json.loads(data_str)\n",
    "                \n",
    "                # get() 메서드를 사용하여 안전하게 데이터에 접근합니다.\n",
    "                choices = chunk.get(\"choices\", [])\n",
    "                if choices:\n",
    "                    delta = choices[0].get(\"delta\", {})\n",
    "                    content = delta.get(\"content\", \"\")\n",
    "                    if content:\n",
    "                        result += content\n",
    "                        # 콘솔에 수신되는 내용을 실시간으로 출력합니다.\n",
    "                        print(content, end=\"\", flush=True)\n",
    "                        time.sleep(0.001)\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"\\n[오류] JSON 디코딩 실패: {data_str}\")\n",
    "    \n",
    "    print()  # 스트림이 끝나면 줄바꿈을 해줍니다.\n",
    "    return result\n",
    "result = stream_parse(response)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m128"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
