{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e82f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "credentials, PROJECT_ID = google.auth.default()\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e34c65-52e5-43c6-83ef-e77f433274cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:16:32.645137Z",
     "iopub.status.busy": "2026-02-04T05:16:32.644493Z",
     "iopub.status.idle": "2026-02-04T05:17:21.196444Z",
     "shell.execute_reply": "2026-02-04T05:17:21.195280Z",
     "shell.execute_reply.started": "2026-02-04T05:16:32.645105Z"
    }
   },
   "outputs": [],
   "source": [
    "#For downloading model to private storage. Use'll use cloud storage as model repository. Tuned model should have uploaded to cloud storage\n",
    "import os\n",
    "hf_repo_id = \"google/gemma-3-4b-it\"\n",
    "print(\"Start downloading\")\n",
    "os.system(f\"gcloud storage cp -r gs://jk-us-public/{hf_repo_id} gs://{PROJECT_ID}/{hf_repo_id}\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b856565-a5f5-4f28-9940-50671fefbe17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:19:07.094289Z",
     "iopub.status.busy": "2026-02-04T05:19:07.093915Z",
     "iopub.status.idle": "2026-02-04T05:19:07.099587Z",
     "shell.execute_reply": "2026-02-04T05:19:07.098670Z",
     "shell.execute_reply.started": "2026-02-04T05:19:07.094256Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Basic configuration\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "LOCATION = \"us-central1\"\n",
    "vLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20260130_0916_RC01\"\n",
    "DEPLOY_TIMEOUT = 3600\n",
    "SERVICE_ACCOUNT = f\"{PROJECT_ID}@{PROJECT_ID}.iam.gserviceaccount.com\"   #SA to access gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e8a7f-5004-4379-bc83-05e631c803c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:19:08.057570Z",
     "iopub.status.busy": "2026-02-04T05:19:08.057212Z",
     "iopub.status.idle": "2026-02-04T05:19:08.062743Z",
     "shell.execute_reply": "2026-02-04T05:19:08.061734Z",
     "shell.execute_reply.started": "2026-02-04T05:19:08.057540Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model configuration\n",
    "MODEL_NAME_V1 = \"gemma-3-4b-it GPU\"\n",
    "MODEL_PATH_V1 = f\"gs://{PROJECT_ID}/{hf_repo_id}\"\n",
    "MODEL_ID = \"/\".join(MODEL_PATH_V1.split(\"/\")[-2:])\n",
    "TENSOR_PARALLEL_SIZE = 1\n",
    "MAX_MODEL_LEN = 32768\n",
    "MACHINE_TYPE = f\"g2-standard-4\" #a2-highgpu-4g, g2-standard-48, a3-edgegpu-8g\n",
    "TPU_TOPOLOGY = None\n",
    "ACCELERATOR_TYPE = \"NVIDIA_L4\" #NVIDIA_TESLA_A100, NVIDIA_L4, NVIDIA_H100_80GB\n",
    "ACCELERATOR_COUNT = TENSOR_PARALLEL_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09dafaf-43be-4169-862d-8285e4769213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:19:11.349574Z",
     "iopub.status.busy": "2026-02-04T05:19:11.349168Z",
     "iopub.status.idle": "2026-02-04T05:22:49.766918Z",
     "shell.execute_reply": "2026-02-04T05:22:49.765981Z",
     "shell.execute_reply.started": "2026-02-04T05:19:11.349516Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Upload model pacakge to vertex model repository\n",
    "from google.cloud import aiplatform\n",
    "model_v1 = aiplatform.Model.upload(\n",
    "    location=LOCATION,\n",
    "    display_name = MODEL_NAME_V1,\n",
    "    #local_model = local_model_v1,\n",
    "    artifact_uri = MODEL_PATH_V1,\n",
    "    #parent_model = prev_model.resource_name,\n",
    "    #is_default_version=True,\n",
    "    #serving_container_environment_variables={\n",
    "    #    \"VERTEX_CPR_MAX_WORKERS\": \"1\",\n",
    "    #    \"PORT\": \"8080\", #server runs on 5000, or 8080 by dafault\n",
    "    #    \"RUST_BACKTRACE\": \"full\", #for stack trace printing,\n",
    "    #},\n",
    "    #serving_container_ports=[8080],\n",
    "    #serving_container_args = [\"--num-shard 1\"]\n",
    "    \n",
    "    #For direct container upload rather then use LocalModel\n",
    "    serving_container_image_uri=vLLM_DOCKER_URI,\n",
    "    serving_container_invoke_route_prefix=\"/*\",\n",
    "    #serving_container_predict_route=\"/v1/chat/completions\",\n",
    "    serving_container_health_route=\"/health\",\n",
    "    serving_container_ports=[8080],\n",
    "    serving_container_environment_variables={\n",
    "        #\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\",\n",
    "        #\"VLLM_USE_V1\": \"1\",\n",
    "        #\"HF_TOKEN\": hf_token\n",
    "        \"MODEL_ID\": MODEL_ID,\n",
    "        \"DEPLOY_SOURCE\": \"API_NATIVE_MODEL\"\n",
    "    },\n",
    "    serving_container_args=[\"python\",\n",
    "                            \"-m\",\n",
    "                            \"vllm.entrypoints.api_server\",\n",
    "                            \"--port=8080\",\n",
    "                            f\"--tensor-parallel-size={TENSOR_PARALLEL_SIZE}\",\n",
    "                            \"--enable-prefix-caching\",\n",
    "                            \"--enable-chunked-prefill\",\n",
    "                            f\"--max-model-len={MAX_MODEL_LEN}\",\n",
    "                            \"--served-model-name=openapi\",\n",
    "                            #\"--enable-request-id-headers\",   #enable to track id from response header\n",
    "                            \"--disable_chunked_mm_input\",\n",
    "                            \"--limit-mm-per-prompt.image=1\",\n",
    "                            \"--limit-mm-per-prompt.video=0\",\n",
    "                            \"--max-num-batched-tokens=32K\",\n",
    "                            f\"--model={MODEL_ID}\",\n",
    "                            #\"--gpu-memory-utilization=0.9\",\n",
    "                            #\"--swap-space=16\",\n",
    "                            #\"--max-num-batched-tokens=512\",\n",
    "                            #\"--enforce-eager\",  #Reduce memory but also slow\n",
    "                            #\"--max-num-seqs=128\", #Temporal for testing\n",
    "                            ],\n",
    "    serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3a722-66f6-46a3-826b-c438f43297f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:22:49.769287Z",
     "iopub.status.busy": "2026-02-04T05:22:49.768679Z",
     "iopub.status.idle": "2026-02-04T05:25:48.629308Z",
     "shell.execute_reply": "2026-02-04T05:25:48.628578Z",
     "shell.execute_reply.started": "2026-02-04T05:22:49.769239Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=f\"{MODEL_NAME_V1} public dedicated test endpoint\",\n",
    "    #labels={\"sample-key\": \"sample-value\"},\n",
    "    location=LOCATION,\n",
    "    dedicated_endpoint_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7fca4c-4895-4113-b8bd-96dc765d7f51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T05:25:48.630781Z",
     "iopub.status.busy": "2026-02-04T05:25:48.630495Z",
     "iopub.status.idle": "2026-02-04T05:57:40.095566Z",
     "shell.execute_reply": "2026-02-04T05:57:40.094645Z",
     "shell.execute_reply.started": "2026-02-04T05:25:48.630753Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Public endpoint with dedicated network\n",
    "response = endpoint.deploy(\n",
    "    model = model_v1,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    tpu_topology=TPU_TOPOLOGY,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    #traffic_percentage=50\n",
    "    #traffic_split={'a':50, 'b':50}\n",
    "    #Configs for GPU\n",
    "    #accelerator_type=\"NVIDIA_L4\",\n",
    "    #machine_type=\"a2-highgpu-1g\",\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    deploy_request_timeout=DEPLOY_TIMEOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ea419-c1b6-4c89-abf3-51c493e29399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T06:04:36.460295Z",
     "iopub.status.busy": "2026-02-04T06:04:36.459937Z",
     "iopub.status.idle": "2026-02-04T06:04:36.467974Z",
     "shell.execute_reply": "2026-02-04T06:04:36.466996Z",
     "shell.execute_reply.started": "2026-02-04T06:04:36.460264Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sample test\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "\n",
    "class CarType(str, Enum):\n",
    "    sedan = \"sedan\"\n",
    "    suv = \"SUV\"\n",
    "    truck = \"Truck\"\n",
    "    coupe = \"Coupe\"\n",
    "\n",
    "class CarDescription(BaseModel):\n",
    "    brand: str\n",
    "    model: str\n",
    "    car_type: CarType\n",
    "\n",
    "prediction_input = {\n",
    "    \"stream\": False,\n",
    "    \"chat_template_kwargs\": {\"enable_thinking\": False},\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Generate a JSON with the brand, model and car_type of the most iconic car from the 90's\"\n",
    "        }\n",
    "    ],\n",
    "    \"structured_outputs\": {\"json\": CarDescription.model_json_schema()}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf298deb-c5a3-40fd-8b5b-6fef6d707400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T06:04:37.637701Z",
     "iopub.status.busy": "2026-02-04T06:04:37.637351Z",
     "iopub.status.idle": "2026-02-04T06:04:39.525952Z",
     "shell.execute_reply": "2026-02-04T06:04:39.524818Z",
     "shell.execute_reply.started": "2026-02-04T06:04:37.637671Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Public and dedicated endpoint predict\n",
    "from google.cloud import aiplatform\n",
    "#ENDPOINT_ID = \"4304072351789613056\"\n",
    "#endpoint = aiplatform.Endpoint(ENDPOINT_ID, location=LOCATION)\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    #\"x-request-id\": \"ebb94475-1ca2-4e4b-baa3-8d039c0e616e\", #works when --enable-request-id-headers option enabled\n",
    "    \"x-vertex-ai-timeout-ms\": \"60000\"\n",
    "}\n",
    "response = endpoint.invoke(request_path=\"/v1/chat/completions\", headers=headers, body=json.dumps(prediction_input, indent=2).encode('utf-8'))\n",
    "print(response.headers)\n",
    "print(response)\n",
    "print(response.json()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ba041-829d-403c-9ba7-0750646a7792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T06:05:13.015194Z",
     "iopub.status.busy": "2026-02-04T06:05:13.014780Z",
     "iopub.status.idle": "2026-02-04T06:05:19.791855Z",
     "shell.execute_reply": "2026-02-04T06:05:19.790757Z",
     "shell.execute_reply.started": "2026-02-04T06:05:13.015164Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"stream\": False,\n",
    "    \"chat_template_kwargs\": {\"enable_thinking\": False},\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": \"What animal is on the candy?\"\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "response = endpoint.invoke(request_path=\"/v1/chat/completions\", headers=headers, body=json.dumps(payload, indent=2).encode('utf-8'))\n",
    "print(response.headers)\n",
    "print(response)\n",
    "print(response.json()['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m139",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m139"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
